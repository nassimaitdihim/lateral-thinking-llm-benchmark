{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12101796,"sourceType":"datasetVersion","datasetId":7618719}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# RoBERTa BRAINTEASER Implementation - Comprehensive Notebook\n# SemEval 2024 Task 9: Lateral Thinking Puzzles\n\nimport os\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport logging\nfrom tqdm.notebook import tqdm\nfrom typing import List, Dict, Tuple, Any, Optional\nfrom dataclasses import dataclass\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import RobertaTokenizer, RobertaForMaskedLM\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML, Markdown\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nprint(\"üß† RoBERTa BRAINTEASER Implementation\")\nprint(\"=\" * 50)\nprint(\"SemEval 2024 Task 9: Lateral Thinking Puzzles\")\nprint(\"Model: RoBERTa-Large with Masked Language Modeling\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:15.771179Z","iopub.execute_input":"2025-06-09T21:49:15.771475Z","iopub.status.idle":"2025-06-09T21:49:15.777837Z","shell.execute_reply.started":"2025-06-09T21:49:15.771455Z","shell.execute_reply":"2025-06-09T21:49:15.777287Z"}},"outputs":[{"name":"stdout","text":"üß† RoBERTa BRAINTEASER Implementation\n==================================================\nSemEval 2024 Task 9: Lateral Thinking Puzzles\nModel: RoBERTa-Large with Masked Language Modeling\n==================================================\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# =====================================\n# 1. CONFIGURATION AND SETUP\n# =====================================\n\n@dataclass\nclass Config:\n    \"\"\"Configuration class for RoBERTa BRAINTEASER implementation\"\"\"\n\n    # Model configuration\n    model_name: str = \"roberta-large\"\n    device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    max_sequence_per_time: int = 80\n\n    # Data paths (to be set by user)\n    sentence_data_path: str = \"/kaggle/input/puzzle-data/SP-train.npy\"\n    wordplay_data_path: str = \"/kaggle/input/puzzle-data/WP-train.npy\"\n\n    # Evaluation settings\n    lowercase_choices: bool = True\n    batch_processing: bool = True\n\n    def __post_init__(self):\n        if not torch.cuda.is_available() and self.device.startswith(\"cuda\"):\n            self.device = \"cpu\"\n            logger.warning(\"CUDA not available, switching to CPU\")\n\nconfig = Config()\nprint(f\"üì± Device: {config.device}\")\nprint(f\"ü§ñ Model: {config.model_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:15.786365Z","iopub.execute_input":"2025-06-09T21:49:15.786627Z","iopub.status.idle":"2025-06-09T21:49:15.795636Z","shell.execute_reply.started":"2025-06-09T21:49:15.786611Z","shell.execute_reply":"2025-06-09T21:49:15.795120Z"}},"outputs":[{"name":"stdout","text":"üì± Device: cuda:0\nü§ñ Model: roberta-large\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# =====================================\n# 2. MODEL INITIALIZATION\n# =====================================\n\nclass RoBERTaBrainTeaserModel:\n    \"\"\"RoBERTa model wrapper for BRAINTEASER evaluation\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.tokenizer = None\n        self.model = None\n        self.device = config.device\n\n    def load_model(self):\n        \"\"\"Load RoBERTa tokenizer and model\"\"\"\n        try:\n            print(\"üîÑ Loading RoBERTa model...\")\n            self.tokenizer = RobertaTokenizer.from_pretrained(self.config.model_name)\n            self.model = RobertaForMaskedLM.from_pretrained(self.config.model_name)\n            self.model.to(self.device)\n            self.model.eval()\n            print(\"‚úÖ Model loaded successfully!\")\n\n            # Set pad token if not available\n            if self.tokenizer.pad_token_id is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        except Exception as e:\n            logger.error(f\"Error loading model: {e}\")\n            raise e\n\n    def get_model_info(self):\n        \"\"\"Display model information\"\"\"\n        if self.model is None:\n            print(\"‚ùå Model not loaded yet. Call load_model() first.\")\n            return\n\n        num_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n\n        info = {\n            \"Model Name\": self.config.model_name,\n            \"Device\": self.device,\n            \"Total Parameters\": f\"{num_params:,}\",\n            \"Trainable Parameters\": f\"{trainable_params:,}\",\n            \"Vocab Size\": self.tokenizer.vocab_size,\n            \"Max Position Embeddings\": self.model.config.max_position_embeddings,\n        }\n\n        print(\"üîç Model Information:\")\n        print(\"-\" * 30)\n        for key, value in info.items():\n            print(f\"{key}: {value}\")\n\n# Initialize model\nroberta_model = RoBERTaBrainTeaserModel(config)\nroberta_model.load_model()\nroberta_model.get_model_info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:15.812070Z","iopub.execute_input":"2025-06-09T21:49:15.812247Z","iopub.status.idle":"2025-06-09T21:49:17.324850Z","shell.execute_reply.started":"2025-06-09T21:49:15.812234Z","shell.execute_reply":"2025-06-09T21:49:17.324238Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading RoBERTa model...\n‚úÖ Model loaded successfully!\nüîç Model Information:\n------------------------------\nModel Name: roberta-large\nDevice: cuda:0\nTotal Parameters: 355,412,057\nTrainable Parameters: 355,412,057\nVocab Size: 50265\nMax Position Embeddings: 514\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# =====================================\n# 3. DATA LOADING AND PREPROCESSING\n# =====================================\n\nclass BrainTeaserDataLoader:\n    \"\"\"Enhanced data loader for BRAINTEASER dataset\"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.sentence_data = []\n        self.wordplay_data = []\n        self.all_data = []\n\n    def load_data(self) -> bool:\n        \"\"\"Load BRAINTEASER data from numpy files\"\"\"\n        try:\n            # Load sentence puzzles\n            if os.path.exists(self.config.sentence_data_path):\n                sentence_raw = np.load(self.config.sentence_data_path, allow_pickle=True)\n                self.sentence_data = self._process_raw_data(sentence_raw, \"SP\")\n                print(f\"‚úÖ Loaded {len(self.sentence_data)} Sentence Puzzle examples\")\n            else:\n                print(f\"‚ö†Ô∏è Sentence data not found at {self.config.sentence_data_path}\")\n\n            # Load word puzzles\n            if os.path.exists(self.config.wordplay_data_path):\n                wordplay_raw = np.load(self.config.wordplay_data_path, allow_pickle=True)\n                self.wordplay_data = self._process_raw_data(wordplay_raw, \"WP\")\n                print(f\"‚úÖ Loaded {len(self.wordplay_data)} Word Puzzle examples\")\n            else:\n                print(f\"‚ö†Ô∏è Wordplay data not found at {self.config.wordplay_data_path}\")\n\n            # Combine all data\n            self.all_data = self.sentence_data + self.wordplay_data\n            print(f\"üìä Total examples: {len(self.all_data)}\")\n\n            return len(self.all_data) > 0\n\n        except Exception as e:\n            logger.error(f\"Error loading data: {e}\")\n            return False\n\n    def _process_raw_data(self, raw_data: np.ndarray, task_type: str) -> List[Dict]:\n        \"\"\"Process raw numpy data into structured format\"\"\"\n        processed = []\n\n        for item in raw_data:\n            if hasattr(item, 'item'):\n                item = item.item()\n\n            # Ensure required fields exist\n            if not all(key in item for key in ['question', 'choice_list', 'label']):\n                continue\n\n            processed_item = {\n                'id': item.get('id', f\"{task_type}_{len(processed)}\"),\n                'question': item['question'],\n                'choice_list': item['choice_list'],\n                'label': item['label'],\n                'task_type': task_type,\n                'answer': item.get('answer', ''),\n                'distractors': item.get('distractor', [])\n            }\n\n            processed.append(processed_item)\n\n        return processed\n\n    def get_data_statistics(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive data statistics\"\"\"\n        if not self.all_data:\n            return {}\n\n        stats = {\n            'total_examples': len(self.all_data),\n            'sentence_puzzles': len(self.sentence_data),\n            'word_puzzles': len(self.wordplay_data),\n            'task_distribution': {\n                'SP': len(self.sentence_data) / len(self.all_data) * 100,\n                'WP': len(self.wordplay_data) / len(self.all_data) * 100\n            }\n        }\n\n        # Question length statistics\n        all_q_lengths = [len(item['question'].split()) for item in self.all_data]\n        sp_q_lengths = [len(item['question'].split()) for item in self.sentence_data]\n        wp_q_lengths = [len(item['question'].split()) for item in self.wordplay_data]\n\n        stats['question_length'] = {\n            'overall': {'mean': np.mean(all_q_lengths), 'std': np.std(all_q_lengths)},\n            'sentence_puzzles': {'mean': np.mean(sp_q_lengths), 'std': np.std(sp_q_lengths)} if sp_q_lengths else {},\n            'word_puzzles': {'mean': np.mean(wp_q_lengths), 'std': np.std(wp_q_lengths)} if wp_q_lengths else {}\n        }\n\n        # Answer distribution\n        answer_distribution = {}\n        for item in self.all_data:\n            label = item['label']\n            answer_distribution[label] = answer_distribution.get(label, 0) + 1\n\n        stats['answer_distribution'] = answer_distribution\n\n        return stats\n\n    def display_examples(self, num_examples: int = 3):\n        \"\"\"Display example questions from the dataset\"\"\"\n        if not self.all_data:\n            print(\"‚ùå No data loaded\")\n            return\n\n        print(\"üìã Example Questions:\")\n        print(\"=\" * 50)\n\n        # Show examples from each task type\n        sp_examples = [item for item in self.all_data if item['task_type'] == 'SP'][:num_examples]\n        wp_examples = [item for item in self.all_data if item['task_type'] == 'WP'][:num_examples]\n\n        for i, example in enumerate(sp_examples):\n            print(f\"\\nüß© Sentence Puzzle {i+1}:\")\n            print(f\"Question: {example['question']}\")\n            print(\"Choices:\")\n            for j, choice in enumerate(example['choice_list']):\n                marker = \"‚úÖ\" if j == example['label'] else \"  \"\n                print(f\"  {marker} ({chr(65+j)}) {choice}\")\n            print(f\"Correct Answer: {example.get('answer', 'N/A')}\")\n\n        for i, example in enumerate(wp_examples):\n            print(f\"\\nüéØ Word Puzzle {i+1}:\")\n            print(f\"Question: {example['question']}\")\n            print(\"Choices:\")\n            for j, choice in enumerate(example['choice_list']):\n                marker = \"‚úÖ\" if j == example['label'] else \"  \"\n                print(f\"  {marker} ({chr(65+j)}) {choice}\")\n            print(f\"Correct Answer: {example.get('answer', 'N/A')}\")\n\n# Load data\ndata_loader = BrainTeaserDataLoader(config)\ndata_loaded = data_loader.load_data()\n\nif data_loaded:\n    # Display statistics\n    stats = data_loader.get_data_statistics()\n    print(\"\\nüìä Dataset Statistics:\")\n    print(\"-\" * 30)\n    print(f\"Total Examples: {stats['total_examples']}\")\n    print(f\"Sentence Puzzles: {stats['sentence_puzzles']} ({stats['task_distribution']['SP']:.1f}%)\")\n    print(f\"Word Puzzles: {stats['word_puzzles']} ({stats['task_distribution']['WP']:.1f}%)\")\n    print(f\"Average Question Length: {stats['question_length']['overall']['mean']:.1f} ¬± {stats['question_length']['overall']['std']:.1f} words\")\n\n    # Display examples\n    data_loader.display_examples()\nelse:\n    print(\"‚ùå Please ensure data files are available in the specified paths\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:17.326250Z","iopub.execute_input":"2025-06-09T21:49:17.326483Z","iopub.status.idle":"2025-06-09T21:49:17.358857Z","shell.execute_reply.started":"2025-06-09T21:49:17.326467Z","shell.execute_reply":"2025-06-09T21:49:17.358068Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded 507 Sentence Puzzle examples\n‚úÖ Loaded 396 Word Puzzle examples\nüìä Total examples: 903\n\nüìä Dataset Statistics:\n------------------------------\nTotal Examples: 903\nSentence Puzzles: 507 (56.1%)\nWord Puzzles: 396 (43.9%)\nAverage Question Length: 24.5 ¬± 20.0 words\nüìã Example Questions:\n==================================================\n\nüß© Sentence Puzzle 1:\nQuestion: Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?\nChoices:\n     (A) Some daughters get married and have their own family.\n  ‚úÖ (B) Each daughter shares the same brother.\n     (C) Some brothers were not loved by family and moved away.\n     (D) None of above.\nCorrect Answer: Each daughter shares the same brother.\n\nüß© Sentence Puzzle 2:\nQuestion: The six daughters of Mr. and Mrs. Mustard each have one brother. However, the family only consists of nine people; how is that possible?\nChoices:\n     (A) Some brothers were not loved by family and moved away.\n     (B) Some daughters get married and have their own family.\n  ‚úÖ (C) Each daughter shares the same brother.\n     (D) None of above.\nCorrect Answer: Each daughter shares the same brother.\n\nüß© Sentence Puzzle 3:\nQuestion: A chess team has five players, and each player has one coach. But there are only six participants in the team. How is that possible?\nChoices:\n  ‚úÖ (A) Each player shares the same coach.\n     (B) Some players are backups and not allowed to play.\n     (C) Some coaches get a raise.\n     (D) None of above.\nCorrect Answer: Each player shares the same coach.\n\nüéØ Word Puzzle 1:\nQuestion: How do you spell COW in thirteen letters?\nChoices:\n     (A) SEE OH DEREFORD\n  ‚úÖ (B) SEE O DOUBLE YOU.\n     (C) COWCOWCOWCOWW\n     (D) None of above.\nCorrect Answer: SEE O DOUBLE YOU.\n\nüéØ Word Puzzle 2:\nQuestion: In thirteen letters, how do you spell COW?\nChoices:\n     (A) SEE OH DEREFORD\n     (B) COWCOWCOWCOWW\n  ‚úÖ (C) SEE O DOUBLE YOU.\n     (D) None of above.\nCorrect Answer: SEE O DOUBLE YOU.\n\nüéØ Word Puzzle 3:\nQuestion: How do you spell COB in seven letters?\nChoices:\n     (A) COBCOBB\n     (B) COBBLER\n  ‚úÖ (C) SEE O BEE\n     (D) None of above.\nCorrect Answer: SEE O BEE\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# =====================================\n# 4. CORE SCORING FUNCTIONS\n# =====================================\n\nclass RoBERTaScorer:\n    \"\"\"Core scoring functionality for RoBERTa BRAINTEASER evaluation\"\"\"\n\n    def __init__(self, model_wrapper: RoBERTaBrainTeaserModel):\n        self.model_wrapper = model_wrapper\n        self.tokenizer = model_wrapper.tokenizer\n        self.model = model_wrapper.model\n        self.device = model_wrapper.device\n        self.config = model_wrapper.config\n\n    def score_question(self, question: str, choices: List[str]) -> int:\n        \"\"\"\n        Score a single question with multiple choices using masked language modeling\n\n        Args:\n            question: The question text\n            choices: List of answer choices\n\n        Returns:\n            Index of the predicted choice (0-based)\n        \"\"\"\n        # Preprocess choices\n        if self.config.lowercase_choices:\n            processed_choices = [choice[0].lower() + choice[1:] if choice else choice\n                               for choice in choices]\n        else:\n            processed_choices = choices\n\n        # Get pad token ID\n        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0\n\n        # Encode question and choices\n        question_ids = self.tokenizer.encode(question)\n        choice_ids = [self.tokenizer.encode(choice, add_prefix_space=True)[1:-1]\n                     for choice in processed_choices]\n\n        # Create sequences for each choice\n        sequences = [question_ids[:-1] + choice_ids[i] + [self.tokenizer.sep_token_id]\n                    for i in range(len(choice_ids))]\n\n        # Create label IDs for loss calculation\n        label_ids = [[-100] + text[1:-1] + [-100] for text in sequences]\n\n        # Prepare input tensors\n        sequences, label_ids, attention_mask = self._prepare_input(sequences, label_ids, pad_token_id)\n\n        # Get prediction\n        prediction = self._token_wise_scoring(sequences, label_ids, attention_mask)\n\n        return prediction\n\n    def _prepare_input(self, sequences: List[List[int]], label_ids: List[List[int]],\n                      pad_token_id: int) -> Tuple[List[List[int]], List[List[int]], np.ndarray]:\n        \"\"\"Prepare input sequences with padding and attention masks\"\"\"\n        max_length = max([len(text) for text in sequences])\n        attention_mask = np.zeros((len(sequences), max_length))\n\n        for i in range(len(sequences)):\n            attention_mask[i][:len(sequences[i])] = 1\n\n        # Pad sequences\n        sequences = [text + [pad_token_id] * (max_length - len(text)) for text in sequences]\n        label_ids = [text + [-100] * (max_length - len(text)) for text in label_ids]\n\n        return sequences, label_ids, attention_mask\n\n    def _token_wise_scoring(self, sequences: List[List[int]], label_ids: List[List[int]],\n                           attention_mask: np.ndarray) -> int:\n        \"\"\"Perform token-wise scoring using masked language modeling\"\"\"\n        choice_losses = [0 for _ in range(len(sequences))]\n\n        for i in range(len(sequences)):\n            curr_label_ids = label_ids[i]\n            tmp_seq_list = []\n            tmp_label_list = []\n            tmp_attention_mask = []\n\n            # Create masked versions for each non-ignored token\n            for j, label in enumerate(curr_label_ids):\n                if label == -100:\n                    continue\n\n                # Create masked sequence\n                masked_seq = sequences[i][:j] + [self.tokenizer.mask_token_id] + sequences[i][j+1:]\n                label_seq = [-100] * j + sequences[i][j:j+1] + [-100] * (len(sequences[i]) - j - 1)\n\n                tmp_seq_list.append(torch.tensor(masked_seq).long().to(self.device))\n                tmp_label_list.append(torch.tensor(label_seq).long().to(self.device))\n                tmp_attention_mask.append(torch.tensor(attention_mask[i]).long().to(self.device))\n\n            if not tmp_seq_list:\n                continue\n\n            # Stack tensors\n            tmp_seq_list = torch.stack(tmp_seq_list)\n            tmp_label_list = torch.stack(tmp_label_list)\n            tmp_attention_mask = torch.stack(tmp_attention_mask)\n\n            # Compute loss in batches\n            if len(tmp_seq_list) < self.config.max_sequence_per_time:\n                loss = self._get_lm_score(tmp_seq_list, tmp_label_list, tmp_attention_mask)\n            else:\n                loss = []\n                for chunk_start in range(0, len(tmp_seq_list), self.config.max_sequence_per_time):\n                    chunk_end = chunk_start + self.config.max_sequence_per_time\n                    chunk_loss = self._get_lm_score(\n                        tmp_seq_list[chunk_start:chunk_end],\n                        tmp_label_list[chunk_start:chunk_end],\n                        tmp_attention_mask[chunk_start:chunk_end]\n                    )\n                    loss.append(chunk_loss)\n                loss = np.concatenate(loss)\n\n            # Average loss for this choice\n            choice_losses[i] = sum(loss) / len(loss) if len(loss) > 0 else float('inf')\n\n        # Return index of choice with minimum loss\n        return choice_losses.index(min(choice_losses))\n\n    def _get_lm_score(self, batch: torch.Tensor, label_ids: torch.Tensor,\n                     attention_mask: torch.Tensor) -> np.ndarray:\n        \"\"\"Get cross-entropy loss for a batch of sequences\"\"\"\n        with torch.no_grad():\n            num_choices, max_length = batch.shape\n            label_ids_flat = label_ids.view(-1)\n\n            # Forward pass\n            outputs = self.model(batch, attention_mask=attention_mask)\n            lm_logits = outputs.logits.view(-1, outputs.logits.size(-1))\n\n            # Compute loss\n            loss_fct = CrossEntropyLoss(reduction=\"none\")\n            loss = loss_fct(lm_logits, label_ids_flat)\n            loss = loss.view(num_choices, -1).sum(1).cpu().numpy()\n\n        return loss\n\n# Initialize scorer\nscorer = RoBERTaScorer(roberta_model)\nprint(\"‚úÖ RoBERTa scorer initialized successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:17.359725Z","iopub.execute_input":"2025-06-09T21:49:17.360323Z","iopub.status.idle":"2025-06-09T21:49:17.375389Z","shell.execute_reply.started":"2025-06-09T21:49:17.360298Z","shell.execute_reply":"2025-06-09T21:49:17.374802Z"}},"outputs":[{"name":"stdout","text":"‚úÖ RoBERTa scorer initialized successfully!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# =====================================\n# 5. EVALUATION FUNCTIONS\n# =====================================\n\nclass BrainTeaserEvaluator:\n    \"\"\"Comprehensive evaluation for BRAINTEASER tasks\"\"\"\n\n    def __init__(self, scorer: RoBERTaScorer):\n        self.scorer = scorer\n        self.results = []\n\n    def evaluate_dataset(self, data: List[Dict], show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"Evaluate the entire dataset\"\"\"\n        predictions = []\n        ground_truth = []\n        detailed_results = []\n\n        iterator = tqdm(data, desc=\"Evaluating\") if show_progress else data\n\n        for sample in iterator:\n            question = sample['question']\n            choices = sample['choice_list']\n            true_label = sample['label']\n\n            # Get prediction\n            prediction = self.scorer.score_question(question, choices)\n\n            predictions.append(prediction)\n            ground_truth.append(true_label)\n\n            # Store detailed result\n            result_item = sample.copy()\n            result_item['prediction'] = prediction\n            result_item['correct'] = prediction == true_label\n            result_item['predicted_choice'] = choices[prediction] if prediction < len(choices) else \"Invalid\"\n\n            detailed_results.append(result_item)\n\n        self.results = detailed_results\n\n        # Calculate metrics\n        metrics = self._calculate_metrics(detailed_results)\n\n        return metrics\n\n    def _calculate_metrics(self, results: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n        # Basic accuracy\n        correct = sum(1 for r in results if r['correct'])\n        total = len(results)\n        instance_accuracy = correct / total if total > 0 else 0\n\n        # Task-specific accuracy\n        sp_results = [r for r in results if r['task_type'] == 'SP']\n        wp_results = [r for r in results if r['task_type'] == 'WP']\n\n        sp_accuracy = sum(1 for r in sp_results if r['correct']) / len(sp_results) if sp_results else 0\n        wp_accuracy = sum(1 for r in wp_results if r['correct']) / len(wp_results) if wp_results else 0\n\n        # Group-based accuracy (for adversarial variants)\n        group_accuracy = self._calculate_group_accuracy(results)\n\n        metrics = {\n            'instance_accuracy': instance_accuracy,\n            'sentence_puzzle_accuracy': sp_accuracy,\n            'word_puzzle_accuracy': wp_accuracy,\n            'group_accuracy': group_accuracy,\n            'total_examples': total,\n            'correct_predictions': correct,\n            'sp_examples': len(sp_results),\n            'wp_examples': len(wp_results)\n        }\n\n        return metrics\n\n    def _calculate_group_accuracy(self, results: List[Dict]) -> Dict[str, float]:\n        \"\"\"Calculate group-based accuracy for adversarial variants\"\"\"\n        # Group by base ID (remove variant suffixes)\n        groups = {}\n        for result in results:\n            base_id = result['id'].split('_')[0] if '_' in result['id'] else result['id']\n            base_id = base_id.split('-')[1] if '-' in base_id else base_id\n\n            if base_id not in groups:\n                groups[base_id] = []\n            groups[base_id].append(result)\n\n        # Calculate group accuracy\n        correct_groups = 0\n        total_groups = len(groups)\n\n        group_details = {}\n\n        for group_id, group_results in groups.items():\n            all_correct = all(r['correct'] for r in group_results)\n            if all_correct:\n                correct_groups += 1\n\n            group_details[group_id] = {\n                'all_correct': all_correct,\n                'individual_results': [(r['id'], r['correct']) for r in group_results]\n            }\n\n        group_accuracy = correct_groups / total_groups if total_groups > 0 else 0\n\n        return {\n            'overall': group_accuracy,\n            'correct_groups': correct_groups,\n            'total_groups': total_groups,\n            'details': group_details\n        }\n\n    def display_results(self, metrics: Dict[str, Any]):\n        \"\"\"Display comprehensive evaluation results\"\"\"\n        print(\"\\nüéØ EVALUATION RESULTS\")\n        print(\"=\" * 50)\n\n        print(f\"üìä Overall Performance:\")\n        print(f\"  Instance Accuracy: {metrics['instance_accuracy']:.4f} ({metrics['correct_predictions']}/{metrics['total_examples']})\")\n\n        if metrics['sp_examples'] > 0:\n            print(f\"  Sentence Puzzles: {metrics['sentence_puzzle_accuracy']:.4f} ({sum(1 for r in self.results if r['task_type'] == 'SP' and r['correct'])}/{metrics['sp_examples']})\")\n\n        if metrics['wp_examples'] > 0:\n            print(f\"  Word Puzzles: {metrics['word_puzzle_accuracy']:.4f} ({sum(1 for r in self.results if r['task_type'] == 'WP' and r['correct'])}/{metrics['wp_examples']})\")\n\n        if 'group_accuracy' in metrics and isinstance(metrics['group_accuracy'], dict):\n            group_acc = metrics['group_accuracy']\n            print(f\"  Group Accuracy: {group_acc['overall']:.4f} ({group_acc['correct_groups']}/{group_acc['total_groups']})\")\n\n        # Error analysis\n        self._display_error_analysis()\n\n    def _display_error_analysis(self):\n        \"\"\"Display detailed error analysis\"\"\"\n        if not self.results:\n            return\n\n        errors = [r for r in self.results if not r['correct']]\n\n        print(f\"\\n‚ùå Error Analysis:\")\n        print(f\"  Total Errors: {len(errors)}\")\n\n        if errors:\n            # Error by task type\n            sp_errors = [e for e in errors if e['task_type'] == 'SP']\n            wp_errors = [e for e in errors if e['task_type'] == 'WP']\n\n            print(f\"  Sentence Puzzle Errors: {len(sp_errors)}\")\n            print(f\"  Word Puzzle Errors: {len(wp_errors)}\")\n\n            # Show a few example errors\n            print(f\"\\nüîç Sample Errors:\")\n            for i, error in enumerate(errors[:3]):\n                print(f\"\\n  Error {i+1} ({error['task_type']}):\")\n                print(f\"    Question: {error['question'][:100]}...\")\n                print(f\"    Predicted: {error['predicted_choice']}\")\n                print(f\"    Correct: {error['choice_list'][error['label']]}\")\n\n    def export_results(self, filename: str = \"roberta_results.json\"):\n        \"\"\"Export detailed results to JSON file\"\"\"\n        if not self.results:\n            print(\"‚ùå No results to export\")\n            return\n\n        export_data = {\n            'model': self.scorer.config.model_name,\n            'total_examples': len(self.results),\n            'results': self.results\n        }\n\n        with open(filename, 'w') as f:\n            json.dump(export_data, f, indent=2)\n\n        print(f\"‚úÖ Results exported to {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:17.376737Z","iopub.execute_input":"2025-06-09T21:49:17.377122Z","iopub.status.idle":"2025-06-09T21:49:17.396930Z","shell.execute_reply.started":"2025-06-09T21:49:17.377105Z","shell.execute_reply":"2025-06-09T21:49:17.396381Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# =====================================\n# 6. MAIN EVALUATION EXECUTION\n# =====================================\n\ndef run_roberta_evaluation():\n    \"\"\"Run the complete RoBERTa evaluation on BRAINTEASER\"\"\"\n\n    if not data_loaded:\n        print(\"‚ùå Cannot run evaluation: No data loaded\")\n        return None\n\n    print(\"\\nüöÄ Starting RoBERTa BRAINTEASER Evaluation\")\n    print(\"=\" * 50)\n\n    # Initialize evaluator\n    evaluator = BrainTeaserEvaluator(scorer)\n\n    # Run evaluation\n    metrics = evaluator.evaluate_dataset(data_loader.all_data)\n\n    # Display results\n    evaluator.display_results(metrics)\n\n    # Export results\n    evaluator.export_results()\n\n    return evaluator, metrics\n\n# Run evaluation if data is loaded\nif data_loaded:\n    # Test with a small sample first\n    print(\"\\nüß™ Testing with sample data...\")\n    sample_data = data_loader.all_data[:5]  # Test with first 5 examples\n\n    evaluator = BrainTeaserEvaluator(scorer)\n    sample_metrics = evaluator.evaluate_dataset(sample_data)\n    evaluator.display_results(sample_metrics)\n\n    # Ask user if they want to run full evaluation\n    print(f\"\\n‚ùì Run full evaluation on all {len(data_loader.all_data)} examples?\")\n    print(\"This may take several minutes depending on your hardware.\")\n    print(\"Uncomment the line below to run full evaluation:\")\n    print(\"# full_evaluator, full_metrics = run_roberta_evaluation()\")\nelse:\n    print(\"\\n‚ö†Ô∏è Please load data first to run evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:17.397506Z","iopub.execute_input":"2025-06-09T21:49:17.397718Z","iopub.status.idle":"2025-06-09T21:49:23.130033Z","shell.execute_reply.started":"2025-06-09T21:49:17.397703Z","shell.execute_reply":"2025-06-09T21:49:23.129288Z"}},"outputs":[{"name":"stdout","text":"\nüß™ Testing with sample data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6657d4f81b07428089baff4238d86172"}},"metadata":{}},{"name":"stdout","text":"\nüéØ EVALUATION RESULTS\n==================================================\nüìä Overall Performance:\n  Instance Accuracy: 0.8000 (4/5)\n  Sentence Puzzles: 0.8000 (4/5)\n  Group Accuracy: 0.5000 (1/2)\n\n‚ùå Error Analysis:\n  Total Errors: 1\n  Sentence Puzzle Errors: 1\n  Word Puzzle Errors: 0\n\nüîç Sample Errors:\n\n  Error 1 (SP):\n    Question: Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 peop...\n    Predicted: Some daughters get married and have their own family.\n    Correct: Each daughter shares the same brother.\n\n‚ùì Run full evaluation on all 903 examples?\nThis may take several minutes depending on your hardware.\nUncomment the line below to run full evaluation:\n# full_evaluator, full_metrics = run_roberta_evaluation()\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# =====================================\n# 7. UTILITY FUNCTIONS AND ANALYSIS\n# =====================================\n\ndef analyze_prediction_patterns(evaluator: BrainTeaserEvaluator):\n    \"\"\"Analyze patterns in model predictions\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available for analysis\")\n        return\n\n    print(\"\\nüîç Prediction Pattern Analysis\")\n    print(\"=\" * 40)\n\n    # Analyze answer distribution\n    pred_distribution = {}\n    true_distribution = {}\n\n    for result in evaluator.results:\n        pred = result['prediction']\n        true = result['label']\n\n        pred_distribution[pred] = pred_distribution.get(pred, 0) + 1\n        true_distribution[true] = true_distribution.get(true, 0) + 1\n\n    print(\"üìä Answer Distribution:\")\n    print(\"Position | Predicted | Actual | Difference\")\n    print(\"-\" * 45)\n\n    for i in range(4):\n        pred_count = pred_distribution.get(i, 0)\n        true_count = true_distribution.get(i, 0)\n        diff = pred_count - true_count\n\n        print(f\"    {chr(65+i)}    |    {pred_count:3d}    |  {true_count:3d}   |    {diff:+3d}\")\n\ndef create_results_visualization(evaluator: BrainTeaserEvaluator):\n    \"\"\"Create visualizations of the results\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available for visualization\")\n        return\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    fig.suptitle('RoBERTa BRAINTEASER Results Analysis', fontsize=16, fontweight='bold')\n\n    # Task-wise accuracy\n    sp_results = [r for r in evaluator.results if r['task_type'] == 'SP']\n    wp_results = [r for r in evaluator.results if r['task_type'] == 'WP']\n\n    sp_acc = sum(1 for r in sp_results if r['correct']) / len(sp_results) if sp_results else 0\n    wp_acc = sum(1 for r in wp_results if r['correct']) / len(wp_results) if wp_results else 0\n\n    task_names = ['Sentence Puzzles', 'Word Puzzles']\n    task_accs = [sp_acc, wp_acc]\n\n    axes[0, 0].bar(task_names, task_accs, color=['skyblue', 'lightcoral'])\n    axes[0, 0].set_ylabel('Accuracy')\n    axes[0, 0].set_title('Accuracy by Task Type')\n    axes[0, 0].set_ylim(0, 1)\n\n    # Add accuracy values on bars\n    for i, v in enumerate(task_accs):\n        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n\n    # Prediction distribution\n    pred_counts = [0, 0, 0, 0]\n    for result in evaluator.results:\n        if 0 <= result['prediction'] < 4:\n            pred_counts[result['prediction']] += 1\n\n    positions = ['A', 'B', 'C', 'D']\n    axes[0, 1].bar(positions, pred_counts, color='lightgreen')\n    axes[0, 1].set_ylabel('Count')\n    axes[0, 1].set_title('Prediction Distribution')\n    axes[0, 1].set_xlabel('Answer Choice')\n\n    # Confidence vs Accuracy (using inverse of loss as confidence proxy)\n    correct_results = [r for r in evaluator.results if r['correct']]\n    incorrect_results = [r for r in evaluator.results if not r['correct']]\n\n    axes[1, 0].hist([len(correct_results), len(incorrect_results)],\n                   bins=2, labels=['Correct', 'Incorrect'],\n                   color=['green', 'red'], alpha=0.7)\n    axes[1, 0].set_ylabel('Count')\n    axes[1, 0].set_title('Correct vs Incorrect Predictions')\n    axes[1, 0].legend()\n\n    # Question length vs accuracy\n    question_lengths = [len(r['question'].split()) for r in evaluator.results]\n    accuracies = [1 if r['correct'] else 0 for r in evaluator.results]\n\n    # Bin by question length\n    length_bins = np.arange(0, max(question_lengths) + 5, 5)\n    binned_acc = []\n    bin_centers = []\n\n    for i in range(len(length_bins) - 1):\n        mask = (np.array(question_lengths) >= length_bins[i]) & (np.array(question_lengths) < length_bins[i+1])\n        if np.sum(mask) > 0:\n            bin_acc = np.mean(np.array(accuracies)[mask])\n            binned_acc.append(bin_acc)\n            bin_centers.append((length_bins[i] + length_bins[i+1]) / 2)\n\n    if bin_centers and binned_acc:\n        axes[1, 1].plot(bin_centers, binned_acc, 'o-', color='purple', linewidth=2, markersize=6)\n        axes[1, 1].set_xlabel('Question Length (words)')\n        axes[1, 1].set_ylabel('Accuracy')\n        axes[1, 1].set_title('Accuracy vs Question Length')\n        axes[1, 1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\ndef get_hardest_questions(evaluator: BrainTeaserEvaluator, n: int = 5):\n    \"\"\"Identify the hardest questions based on model performance\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available\")\n        return\n\n    # Get incorrect predictions\n    errors = [r for r in evaluator.results if not r['correct']]\n\n    print(f\"\\nüî• Top {min(n, len(errors))} Hardest Questions:\")\n    print(\"=\" * 60)\n\n    for i, error in enumerate(errors[:n]):\n        print(f\"\\n{i+1}. Question ID: {error['id']} ({error['task_type']})\")\n        print(f\"   Question: {error['question']}\")\n        print(\"   Choices:\")\n        for j, choice in enumerate(error['choice_list']):\n            marker = \"‚ùå\" if j == error['prediction'] else \"‚úÖ\" if j == error['label'] else \"  \"\n            print(f\"     {marker} ({chr(65+j)}) {choice}\")\n        print(f\"   Model predicted: {error['predicted_choice']}\")\n        print(f\"   Correct answer: {error['choice_list'][error['label']]}\")\n        print(\"-\" * 60)\n\ndef compare_task_performance(evaluator: BrainTeaserEvaluator):\n    \"\"\"Compare performance between sentence and word puzzles\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available\")\n        return\n\n    sp_results = [r for r in evaluator.results if r['task_type'] == 'SP']\n    wp_results = [r for r in evaluator.results if r['task_type'] == 'WP']\n\n    print(\"\\nüìä Task Performance Comparison\")\n    print(\"=\" * 40)\n\n    if sp_results:\n        sp_correct = sum(1 for r in sp_results if r['correct'])\n        sp_accuracy = sp_correct / len(sp_results)\n        sp_avg_length = np.mean([len(r['question'].split()) for r in sp_results])\n\n        print(f\"üß© Sentence Puzzles:\")\n        print(f\"   Accuracy: {sp_accuracy:.4f} ({sp_correct}/{len(sp_results)})\")\n        print(f\"   Avg Question Length: {sp_avg_length:.1f} words\")\n        print(f\"   Error Rate: {(1-sp_accuracy)*100:.1f}%\")\n\n    if wp_results:\n        wp_correct = sum(1 for r in wp_results if r['correct'])\n        wp_accuracy = wp_correct / len(wp_results)\n        wp_avg_length = np.mean([len(r['question'].split()) for r in wp_results])\n\n        print(f\"\\nüéØ Word Puzzles:\")\n        print(f\"   Accuracy: {wp_accuracy:.4f} ({wp_correct}/{len(wp_results)})\")\n        print(f\"   Avg Question Length: {wp_avg_length:.1f} words\")\n        print(f\"   Error Rate: {(1-wp_accuracy)*100:.1f}%\")\n\n    if sp_results and wp_results:\n        print(f\"\\nüìà Comparison:\")\n        better_task = \"Sentence Puzzles\" if sp_accuracy > wp_accuracy else \"Word Puzzles\"\n        diff = abs(sp_accuracy - wp_accuracy)\n        print(f\"   Better Performance: {better_task}\")\n        print(f\"   Performance Gap: {diff:.4f} ({diff*100:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:23.130936Z","iopub.execute_input":"2025-06-09T21:49:23.131172Z","iopub.status.idle":"2025-06-09T21:49:23.151220Z","shell.execute_reply.started":"2025-06-09T21:49:23.131155Z","shell.execute_reply":"2025-06-09T21:49:23.150595Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# =====================================\n# 8. ADVANCED ANALYSIS FUNCTIONS\n# =====================================\n\ndef analyze_error_patterns(evaluator: BrainTeaserEvaluator):\n    \"\"\"Detailed analysis of error patterns\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available\")\n        return\n\n    print(\"\\nüîç Detailed Error Pattern Analysis\")\n    print(\"=\" * 50)\n\n    errors = [r for r in evaluator.results if not r['correct']]\n\n    # Pattern 1: Question characteristics that lead to errors\n    print(\"üìù Error-prone Question Characteristics:\")\n\n    # Length analysis\n    error_lengths = [len(r['question'].split()) for r in errors]\n    correct_lengths = [len(r['question'].split()) for r in evaluator.results if r['correct']]\n\n    print(f\"   Avg length of failed questions: {np.mean(error_lengths):.1f} words\")\n    print(f\"   Avg length of correct questions: {np.mean(correct_lengths):.1f} words\")\n\n    # Keyword analysis\n    error_keywords = {}\n    correct_keywords = {}\n\n    common_words = ['what', 'how', 'why', 'when', 'where', 'who', 'which', 'not', 'no', 'never']\n\n    for word in common_words:\n        error_count = sum(1 for r in errors if word.lower() in r['question'].lower())\n        correct_count = sum(1 for r in evaluator.results if r['correct'] and word.lower() in r['question'].lower())\n\n        total_with_word = error_count + correct_count\n        if total_with_word > 0:\n            error_rate = error_count / total_with_word\n            print(f\"   '{word}' error rate: {error_rate:.3f} ({error_count}/{total_with_word})\")\n\n    # Pattern 2: Prediction patterns\n    print(f\"\\nüéØ Prediction Patterns:\")\n    print(\"   Most confused answer pairs:\")\n\n    confusion_matrix = {}\n    for error in errors:\n        true_label = error['label']\n        pred_label = error['prediction']\n        pair = (true_label, pred_label)\n        confusion_matrix[pair] = confusion_matrix.get(pair, 0) + 1\n\n    sorted_confusions = sorted(confusion_matrix.items(), key=lambda x: x[1], reverse=True)\n    for (true_pos, pred_pos), count in sorted_confusions[:5]:\n        print(f\"   {chr(65+true_label)} ‚Üí {chr(65+pred_pos)}: {count} times\")\n\ndef benchmark_against_baselines(evaluator: BrainTeaserEvaluator):\n    \"\"\"Compare RoBERTa performance against known baselines\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available\")\n        return\n\n    print(\"\\nüìä Baseline Comparison\")\n    print(\"=\" * 40)\n\n    # Calculate our metrics\n    total_correct = sum(1 for r in evaluator.results if r['correct'])\n    total_questions = len(evaluator.results)\n    our_accuracy = total_correct / total_questions\n\n    # Known baselines from literature\n    baselines = {\n        'Random Baseline': 0.25,  # 4-choice multiple choice\n        'Human Performance': 0.92,  # From original paper\n        'ChatGPT (Zero-shot)': 0.575,  # Average of SP (60.77%) and WP (56.10%)\n        'Competition Winner': 0.835,  # Average of best results (81.7% SP, 85.4% WP)\n    }\n\n    print(\"Model Comparison:\")\n    print(\"-\" * 30)\n\n    # Add our result\n    baselines['RoBERTa-Large (Ours)'] = our_accuracy\n\n    # Sort by performance\n    sorted_baselines = sorted(baselines.items(), key=lambda x: x[1], reverse=True)\n\n    for model, accuracy in sorted_baselines:\n        bar_length = int(accuracy * 40)  # Scale to 40 characters\n        bar = \"‚ñà\" * bar_length + \"‚ñë\" * (40 - bar_length)\n\n        if model == 'RoBERTa-Large (Ours)':\n            print(f\"üëâ {model:<25} {accuracy:.3f} |{bar}|\")\n        else:\n            print(f\"   {model:<25} {accuracy:.3f} |{bar}|\")\n\n    # Performance analysis\n    human_gap = baselines['Human Performance'] - our_accuracy\n    random_improvement = our_accuracy - baselines['Random Baseline']\n\n    print(f\"\\nüìà Performance Analysis:\")\n    print(f\"   Gap to Human Performance: {human_gap:.3f} ({human_gap*100:.1f}%)\")\n    print(f\"   Improvement over Random: {random_improvement:.3f} ({random_improvement*100:.1f}%)\")\n\n    if our_accuracy > baselines['ChatGPT (Zero-shot)']:\n        chatgpt_improvement = our_accuracy - baselines['ChatGPT (Zero-shot)']\n        print(f\"   ‚úÖ Outperforms ChatGPT by: {chatgpt_improvement:.3f} ({chatgpt_improvement*100:.1f}%)\")\n    else:\n        chatgpt_gap = baselines['ChatGPT (Zero-shot)'] - our_accuracy\n        print(f\"   ‚ùå ChatGPT advantage: {chatgpt_gap:.3f} ({chatgpt_gap*100:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:23.152334Z","iopub.execute_input":"2025-06-09T21:49:23.152568Z","iopub.status.idle":"2025-06-09T21:49:23.171179Z","shell.execute_reply.started":"2025-06-09T21:49:23.152549Z","shell.execute_reply":"2025-06-09T21:49:23.170557Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# =====================================\n# 9. EXPORT AND REPORTING FUNCTIONS\n# =====================================\n\ndef generate_comprehensive_report(evaluator: BrainTeaserEvaluator,\n                                output_file: str = \"roberta_brainteaser_report.html\"):\n    \"\"\"Generate a comprehensive HTML report\"\"\"\n    if not evaluator.results:\n        print(\"‚ùå No results available\")\n        return\n\n    # Calculate metrics\n    total_correct = sum(1 for r in evaluator.results if r['correct'])\n    total_questions = len(evaluator.results)\n    accuracy = total_correct / total_questions\n\n    sp_results = [r for r in evaluator.results if r['task_type'] == 'SP']\n    wp_results = [r for r in evaluator.results if r['task_type'] == 'WP']\n\n    sp_accuracy = sum(1 for r in sp_results if r['correct']) / len(sp_results) if sp_results else 0\n    wp_accuracy = sum(1 for r in wp_results if r['correct']) / len(wp_results) if wp_results else 0\n\n    # Generate HTML report\n    html_content = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>RoBERTa BRAINTEASER Evaluation Report</title>\n        <style>\n            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n            .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}\n            .metric {{ display: inline-block; margin: 10px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}\n            .correct {{ color: green; }}\n            .incorrect {{ color: red; }}\n            .example {{ margin: 10px 0; padding: 10px; border-left: 3px solid #ccc; }}\n            table {{ border-collapse: collapse; width: 100%; }}\n            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n            th {{ background-color: #f2f2f2; }}\n        </style>\n    </head>\n    <body>\n        <div class=\"header\">\n            <h1>üß† RoBERTa BRAINTEASER Evaluation Report</h1>\n            <p>Model: {config.model_name} | Device: {config.device}</p>\n            <p>Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n        </div>\n\n        <h2>üìä Overall Performance</h2>\n        <div class=\"metric\">\n            <h3>Instance Accuracy</h3>\n            <p><strong>{accuracy:.4f}</strong> ({total_correct}/{total_questions})</p>\n        </div>\n\n        <div class=\"metric\">\n            <h3>Sentence Puzzles</h3>\n            <p><strong>{sp_accuracy:.4f}</strong> ({sum(1 for r in sp_results if r['correct'])}/{len(sp_results)})</p>\n        </div>\n\n        <div class=\"metric\">\n            <h3>Word Puzzles</h3>\n            <p><strong>{wp_accuracy:.4f}</strong> ({sum(1 for r in wp_results if r['correct'])}/{len(wp_results)})</p>\n        </div>\n\n        <h2>‚ùå Error Analysis</h2>\n        <p>Total Errors: {len([r for r in evaluator.results if not r['correct']])}</p>\n\n        <h3>Sample Errors:</h3>\n    \"\"\"\n\n    # Add error examples\n    errors = [r for r in evaluator.results if not r['correct']][:5]\n    for i, error in enumerate(errors):\n        html_content += f\"\"\"\n        <div class=\"example\">\n            <strong>Error {i+1} ({error['task_type']}):</strong><br>\n            <strong>Q:</strong> {error['question']}<br>\n            <strong>Predicted:</strong> <span class=\"incorrect\">{error['predicted_choice']}</span><br>\n            <strong>Correct:</strong> <span class=\"correct\">{error['choice_list'][error['label']]}</span>\n        </div>\n        \"\"\"\n\n    # Add detailed results table\n    html_content += \"\"\"\n        <h2>üìã Detailed Results</h2>\n        <table>\n            <tr>\n                <th>ID</th>\n                <th>Task</th>\n                <th>Question</th>\n                <th>Predicted</th>\n                <th>Correct</th>\n                <th>Status</th>\n            </tr>\n    \"\"\"\n\n    for result in evaluator.results[:20]:  # Show first 20 results\n        status = \"‚úÖ\" if result['correct'] else \"‚ùå\"\n        question_preview = result['question'][:50] + \"...\" if len(result['question']) > 50 else result['question']\n\n        html_content += f\"\"\"\n            <tr>\n                <td>{result['id']}</td>\n                <td>{result['task_type']}</td>\n                <td>{question_preview}</td>\n                <td>{result['predicted_choice'][:30]}</td>\n                <td>{result['choice_list'][result['label']][:30]}</td>\n                <td>{status}</td>\n            </tr>\n        \"\"\"\n\n    html_content += \"\"\"\n        </table>\n        <p><em>Showing first 20 results. See JSON export for complete results.</em></p>\n    </body>\n    </html>\n    \"\"\"\n\n    # Write to file\n    with open(output_file, 'w', encoding='utf-8') as f:\n        f.write(html_content)\n\n    print(f\"‚úÖ Comprehensive report saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:23.171855Z","iopub.execute_input":"2025-06-09T21:49:23.172055Z","iopub.status.idle":"2025-06-09T21:49:23.188795Z","shell.execute_reply.started":"2025-06-09T21:49:23.172014Z","shell.execute_reply":"2025-06-09T21:49:23.188067Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# =====================================\n# 10. INTERACTIVE USAGE EXAMPLES\n# =====================================\n\ndef interactive_question_test():\n    \"\"\"Interactive function to test individual questions\"\"\"\n    print(\"\\nüéÆ Interactive Question Testing\")\n    print(\"=\" * 40)\n    print(\"Enter a question and choices to test the RoBERTa model\")\n    print(\"Type 'quit' to exit\")\n\n    while True:\n        print(\"\\n\" + \"-\" * 40)\n        question = input(\"Enter question: \").strip()\n\n        if question.lower() == 'quit':\n            break\n\n        if not question:\n            continue\n\n        print(\"Enter 4 choices (press Enter after each):\")\n        choices = []\n        for i in range(4):\n            choice = input(f\"Choice {chr(65+i)}: \").strip()\n            choices.append(choice)\n\n        if len(choices) == 4 and all(choices):\n            try:\n                prediction = scorer.score_question(question, choices)\n                predicted_choice = choices[prediction]\n\n                print(f\"\\nü§ñ Model Prediction:\")\n                print(f\"   Predicted Answer: ({chr(65+prediction)}) {predicted_choice}\")\n                print(f\"   Confidence Rank: {prediction + 1}/4\")\n\n            except Exception as e:\n                print(f\"‚ùå Error processing question: {e}\")\n        else:\n            print(\"‚ùå Please provide all 4 choices\")\n\ndef quick_start_demo():\n    \"\"\"Quick demonstration of key features\"\"\"\n    print(\"\\nüöÄ RoBERTa BRAINTEASER Quick Start Demo\")\n    print(\"=\" * 50)\n\n    if not data_loaded:\n        print(\"‚ùå Demo requires data to be loaded\")\n        return\n\n    # Show model info\n    roberta_model.get_model_info()\n\n    # Show data stats\n    stats = data_loader.get_data_statistics()\n    print(f\"\\nüìä Loaded {stats['total_examples']} examples\")\n\n    # Test on a few examples\n    print(\"\\nüß™ Testing on sample questions...\")\n    sample_data = data_loader.all_data[:3]\n\n    for i, example in enumerate(sample_data):\n        print(f\"\\nüìù Example {i+1} ({example['task_type']}):\")\n        print(f\"Q: {example['question']}\")\n\n        try:\n            prediction = scorer.score_question(example['question'], example['choice_list'])\n            predicted_choice = example['choice_list'][prediction]\n            correct_choice = example['choice_list'][example['label']]\n            is_correct = prediction == example['label']\n\n            print(f\"Model: ({chr(65+prediction)}) {predicted_choice}\")\n            print(f\"Actual: ({chr(65+example['label'])}) {correct_choice}\")\n            print(f\"Result: {'‚úÖ Correct' if is_correct else '‚ùå Wrong'}\")\n\n        except Exception as e:\n            print(f\"‚ùå Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:23.189606Z","iopub.execute_input":"2025-06-09T21:49:23.190307Z","iopub.status.idle":"2025-06-09T21:49:23.207294Z","shell.execute_reply.started":"2025-06-09T21:49:23.190285Z","shell.execute_reply":"2025-06-09T21:49:23.206529Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# =====================================\n# 11. FINAL EXECUTION AND SUMMARY\n# =====================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéØ ROBERTA BRAINTEASER IMPLEMENTATION READY\")\nprint(\"=\"*60)\n\nif data_loaded:\n    print(f\"‚úÖ Data loaded: {len(data_loader.all_data)} examples\")\n    print(f\"‚úÖ Model ready: {config.model_name}\")\n    print(f\"‚úÖ Device: {config.device}\")\n\n    print(\"\\nüöÄ Available Functions:\")\n    print(\"   run_roberta_evaluation() - Run full evaluation\")\n    print(\"   quick_start_demo() - Quick demonstration\")\n    print(\"   interactive_question_test() - Test custom questions\")\n    print(\"   analyze_prediction_patterns(evaluator) - Analyze patterns\")\n    print(\"   create_results_visualization(evaluator) - Create plots\")\n    print(\"   generate_comprehensive_report(evaluator) - Generate HTML report\")\n\n    print(\"\\nüí° Quick Start:\")\n    print(\"   evaluator, metrics = run_roberta_evaluation()\")\n\n    evaluator, metrics = run_roberta_evaluation()\n    analyze_prediction_patterns(evaluator)\n    generate_comprehensive_report(evaluator)\n\nelse:\n    print(\"‚ùå Data not loaded. Please:\")\n    print(\"   1. Ensure data files exist:\")\n    print(f\"      - {config.sentence_data_path}\")\n    print(f\"      - {config.wordplay_data_path}\")\n    print(\"   2. Update config.sentence_data_path and config.wordplay_data_path\")\n    print(\"   3. Run: data_loader.load_data()\")\n\nprint(\"\\nüìö Implementation Features:\")\nprint(\"   ‚úÖ Clean, organized code structure\")\nprint(\"   ‚úÖ Comprehensive error handling\")\nprint(\"   ‚úÖ Detailed evaluation metrics\")\nprint(\"   ‚úÖ Interactive analysis tools\")\nprint(\"   ‚úÖ Visualization capabilities\")\nprint(\"   ‚úÖ Export functionality\")\nprint(\"   ‚úÖ Baseline comparisons\")\n\nprint(\"\\n\" + \"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T21:49:23.208824Z","iopub.execute_input":"2025-06-09T21:49:23.209107Z","iopub.status.idle":"2025-06-09T22:05:02.813050Z","shell.execute_reply.started":"2025-06-09T21:49:23.209083Z","shell.execute_reply":"2025-06-09T22:05:02.812434Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nüéØ ROBERTA BRAINTEASER IMPLEMENTATION READY\n============================================================\n‚úÖ Data loaded: 903 examples\n‚úÖ Model ready: roberta-large\n‚úÖ Device: cuda:0\n\nüöÄ Available Functions:\n   run_roberta_evaluation() - Run full evaluation\n   quick_start_demo() - Quick demonstration\n   interactive_question_test() - Test custom questions\n   analyze_prediction_patterns(evaluator) - Analyze patterns\n   create_results_visualization(evaluator) - Create plots\n   generate_comprehensive_report(evaluator) - Generate HTML report\n\nüí° Quick Start:\n   evaluator, metrics = run_roberta_evaluation()\n\nüöÄ Starting RoBERTa BRAINTEASER Evaluation\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/903 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3e48a9319104f17b2cf6cdb168e3e5b"}},"metadata":{}},{"name":"stdout","text":"\nüéØ EVALUATION RESULTS\n==================================================\nüìä Overall Performance:\n  Instance Accuracy: 0.3422 (309/903)\n  Sentence Puzzles: 0.4517 (229/507)\n  Word Puzzles: 0.2020 (80/396)\n  Group Accuracy: 0.0789 (15/190)\n\n‚ùå Error Analysis:\n  Total Errors: 594\n  Sentence Puzzle Errors: 278\n  Word Puzzle Errors: 316\n\nüîç Sample Errors:\n\n  Error 1 (SP):\n    Question: Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 peop...\n    Predicted: Some daughters get married and have their own family.\n    Correct: Each daughter shares the same brother.\n\n  Error 2 (SP):\n    Question: A woman shoots publicly at people at a National Park. The park is full of people, but no one gets ki...\n    Predicted: The woman wanted to cause chaos.\n    Correct: The woman was a photographer.\n\n  Error 3 (SP):\n    Question: Tom is a clean freak but he never dries his hair after a shower. How is this possible?...\n    Predicted: None of above.\n    Correct: This man is bald.\n‚úÖ Results exported to roberta_results.json\n\nüîç Prediction Pattern Analysis\n========================================\nüìä Answer Distribution:\nPosition | Predicted | Actual | Difference\n---------------------------------------------\n    A    |    257    |  272   |    -15\n    B    |    245    |  293   |    -48\n    C    |    305    |  285   |    +20\n    D    |     96    |   53   |    +43\n‚úÖ Comprehensive report saved to roberta_brainteaser_report.html\n\nüìö Implementation Features:\n   ‚úÖ Clean, organized code structure\n   ‚úÖ Comprehensive error handling\n   ‚úÖ Detailed evaluation metrics\n   ‚úÖ Interactive analysis tools\n   ‚úÖ Visualization capabilities\n   ‚úÖ Export functionality\n   ‚úÖ Baseline comparisons\n\n============================================================\n","output_type":"stream"}],"execution_count":26}]}